Okay, this is an interesting and powerful workflow! Let's break this down into manageable steps to build your "Feature Symphony" tool.

**Core Idea:**
The tool will take a high-level feature breakdown (provided by you in an XML file after an initial LLM chat), then use an LLM (Gemini) to generate detailed implementation guides for each sub-feature. Finally, it will launch Aider instances in parallel to implement these guides.

**Outline of Implementation Steps:**

1.  **Project Scaffolding & Initial Setup:**
    *   Create the directory structure for the `feature_symphony_tool`.
    *   Add the `dump_repo.sh` script.
    *   Create `config.yaml.template` and `.env.template`.
    *   Write initial `README.md` and `requirements.txt`.

2.  **Core Python Orchestration (`orchestrator.py`):**
    *   Load configuration (`config.yaml`, environment variables).
    *   Parse the input "symphony" XML file to extract feature descriptions.
    *   For each feature:
        *   Construct a prompt for Gemini to generate a detailed "feature slice guide".
        *   Call the Gemini API.
        *   Save the generated guide to the user-specified output directory.
    *   Prepare a JSON structure detailing the Aider tasks (guide file paths, global context files, standard prompt) and print it to `stdout`.

3.  **Aider Launch Script (`launch_aiders.sh`):**
    *   Take a `RUN_ID` and a path to a JSON file (generated by `orchestrator.py`) as input.
    *   Parse the JSON to get task details.
    *   Create and manage a `tmux` session.
    *   For each task, launch an `aider` instance in a new `tmux` window with the specified guide, global context files, and prompt.

4.  **Main Workflow Script (`run_symphony.sh`):**
    *   Handle command-line arguments (path to symphony XML).
    *   Set up the environment (Python venv, install dependencies).
    *   Source `.env` file.
    *   Generate a unique `RUN_ID`.
    *   Execute `orchestrator.py`, capturing its JSON output to a temporary file.
    *   Execute `launch_aiders.sh` with the `RUN_ID` and the path to the JSON task file.

5.  **Standalone Aider Task Runner (`run_single_aider_task.sh`):**
    *   Handle command-line arguments (path to a single pre-existing feature slice guide).
    *   Generate a simple JSON task structure for this single guide.
    *   Execute `launch_aiders.sh` similarly to the main workflow.

6.  **Documentation & Refinement:**
    *   Flesh out `README.md` with detailed setup, usage, and configuration instructions.
    *   Add error handling and logging.
    *   Test and iterate on prompts and Aider commands.

Let's start building!

---

**Slice 1: Project Scaffolding & Initial Setup**

I'll create the following file structure and initial content.

**Directory Structure:**
```
feature_symphony_tool/
├── bin/
│   ├── dump_repo.sh
│   └── launch_aiders.sh  # Placeholder for now
├── src/
│   ├── orchestrator.py   # Placeholder for now
│   └── utils.py          # Placeholder for now (e.g., for API call logic)
├── config/
│   └── config.yaml.template
├── .env.template
├── requirements.txt
├── run_symphony.sh         # Placeholder for now
├── run_single_aider_task.sh # Placeholder for now
├── .gitignore
└── README.md
```

**1. `feature_symphony_tool/bin/dump_repo.sh`**
(This is the script you provided)
```bash
#!/bin/bash
# Get the root directory of the git repository
REPO_ROOT=$(git rev-parse --show-toplevel)
# Set output file name (in the REPO_ROOT)
OUTPUT_FILE="$REPO_ROOT/repo_contents.txt"

# Clear the file if it exists or create a new empty file
> "$OUTPUT_FILE"

# Define exclusion patterns (relative to REPO_ROOT)
# Add more patterns as needed
EXCLUDES=(
  "docs/guides/*"       # Example: exclude generated guides
  "docs/context/*"      # Example: exclude other context files
  "pnpm-lock.yaml"
  ".gitignore"
  "LICENSE"
  "*.lock"
  "node_modules/*"
  "dist/*"
  "build/*"
  ".venv/*"
  "__pycache__/*"
  ".DS_Store"
  "feature_symphony_tool/runs/*" # Exclude tool's own run artifacts
  "repo_contents.txt" # Exclude the output file itself
)

# Function to check if a file matches any exclude pattern
should_exclude() {
  local file_to_check="$1"
  for pattern in "${EXCLUDES[@]}"; do
    if [[ "$file_to_check" == $pattern || "$file_to_check" == */$pattern ]]; then
      # Handle direct match and directory prefix match for patterns like "node_modules/*"
      if [[ "$pattern" == *"*"* ]]; then # If pattern contains wildcard
         if [[ "$file_to_check" == $pattern ]]; then
            return 0 # Exclude
         fi
      elif [[ "$file_to_check" == "$pattern" ]]; then # Exact match
            return 0 # Exclude
      fi
    fi
  done
  return 1  # Should not exclude
}

echo "Dumping repository contents to $OUTPUT_FILE..."
echo "Excluding patterns: ${EXCLUDES[*]}"

# Get list of all committed files, excluding deleted ones
# Using git ls-files -co --exclude-standard to respect .gitignore and get cached/other files
# Then apply custom exclusion list
git ls-files -co --exclude-standard | while read -r file; do
    relative_file_path="$file" # Path is already relative to REPO_ROOT

    if should_exclude "$relative_file_path"; then
        # echo "Excluding: $relative_file_path" # Uncomment for debugging
        continue
    fi

    # Check if file exists (not deleted)
    if [ -f "$REPO_ROOT/$relative_file_path" ]; then
        # Add file name as header
        echo -e "\n\n===== $relative_file_path =====\n" >> "$OUTPUT_FILE"

        # Append file contents
        cat "$REPO_ROOT/$relative_file_path" >> "$OUTPUT_FILE"
    fi
done

echo "Repository contents dumped to $OUTPUT_FILE"
```
*Self-correction:* Modified `dump_repo.sh` slightly:
    *   Output file path now explicitly uses `$REPO_ROOT`.
    *   Improved exclusion logic to better handle patterns and paths.
    *   Added more common exclusion patterns.
    *   Ensured `repo_contents.txt` itself is excluded.
    *   Switched to `git ls-files -co --exclude-standard` for a better base list of files and then apply custom exclusions.

**2. `feature_symphony_tool/config/config.yaml.template`**
```yaml
# feature_symphony_tool/config/config.yaml
# Copy this to config.yaml and fill in your values.

# Gemini API Configuration
# Your Gemini API Key.
# IMPORTANT: It's highly recommended to set this via the GEMINI_API_KEY environment variable
# (e.g., in an .env file) instead of hardcoding it here for security.
# If gemini_api_key is null or empty, the tool will expect GEMINI_API_KEY environment variable.
gemini_api_key: null

# Gemini model to use for generating feature slice guides.
# Example: "gemini-1.5-pro-latest", "gemini-pro"
gemini_model_guide_generation: "gemini-1.5-pro-latest"

# Aider Configuration
# List of global context files to always include with Aider.
# These paths are relative to the root of the project where you run the symphony tool.
aider_global_context_files:
  - "README.md"
  # - "src/core_logic.py"
  # - "docs/architecture.md"

# Output Configuration
# Directory where the generated feature_slice_guide_*.md files will be saved.
# This path is relative to the root of the project where you run the symphony tool.
guides_output_directory: "docs/feature_guides" # Example: "project_docs/generated_guides"

# Tmux session name prefix for Aider runs
# The actual session name will be: {tmux_session_prefix}_{RUN_ID}
tmux_session_prefix: "symphony_aider"

# --- Advanced ---
# Optional: Directory within feature_symphony_tool to store logs and intermediate files for each run.
# If empty, no run-specific subdirectories are created by the tool for its own artifacts.
# Tool's internal run artifacts directory (relative to feature_symphony_tool location)
# Example: "runs" will create feature_symphony_tool/runs/RUN_ID/...
tool_run_artifacts_dir: "runs"
```

**3. `feature_symphony_tool/.env.template`**
```env
# Copy this file to .env in the feature_symphony_tool directory
# or ensure these environment variables are set in your shell.
# This file is typically gitignored.

# Gemini API Key
GEMINI_API_KEY="your_google_gemini_api_key_here"

# Optional: If Aider needs specific API keys for its models (e.g. OPENAI_API_KEY for GPT models)
# Aider will usually pick these up from its own config or standard environment variables.
# OPENAI_API_KEY="your_openai_api_key_here"
# ANTHROPIC_API_KEY="your_anthropic_api_key_here"
```

**4. `feature_symphony_tool/requirements.txt`**
```txt
# Core dependencies
PyYAML>=6.0
requests>=2.28.0
python-dotenv>=1.0.0
google-generativeai>=0.5.0 # For Gemini API

# For slugifying filenames (optional, can implement manually)
python-slugify>=8.0.0

# Utility for command line interfaces
click>=8.0.0
```

**5. `feature_symphony_tool/.gitignore`**
```gitignore
# Python
__pycache__/
*.py[cod]
*$py.class
*.egg-info/
.env
*.egg
dist/
build/
*.ini
*.log
*.db
*.sqlite3

# feature_symphony_tool specific
config/config.yaml # Encourage users to use config.yaml.template and not commit their actual config
runs/ # Tool's internal run artifacts

# Virtual environment
.venv/
venv/
ENV/
```

**6. `feature_symphony_tool/README.md`** (Initial Draft)
```markdown
# Feature Symphony Tool

This tool automates the process of breaking down large features into smaller, manageable slices, generating implementation guides for these slices using Google's Gemini API, and then launching Aider instances to implement these guides.

## Prerequisites

1.  **Python**: Version 3.8+
2.  **Git**: For the `dump_repo.sh` script.
3.  **tmux**: For running Aider instances in parallel. (e.g., `sudo apt install tmux` or `brew install tmux`)
4.  **jq**: For parsing JSON in shell scripts. (e.g., `sudo apt install jq` or `brew install jq`)
5.  **Aider**: Ensure `aider` is installed and configured. See Aider's documentation.
6.  **Google Gemini API Key**: You'll need an API key for Gemini.

## Setup

1.  **Clone/Place the Tool**:
    Place the `feature_symphony_tool` directory into your project or a preferred location.

2.  **Configure the Tool**:
    *   Navigate to the `feature_symphony_tool` directory.
    *   Copy `config/config.yaml.template` to `config/config.yaml`.
    *   Edit `config/config.yaml` to set your `gemini_model_guide_generation`, `aider_global_context_files`, and `guides_output_directory` (relative to your main project root).
    *   Copy `.env.template` to `.env`.
    *   Edit `.env` to add your `GEMINI_API_KEY`.
        ```bash
        cd feature_symphony_tool
        cp config/config.yaml.template config/config.yaml
        cp .env.template .env
        # Now edit config/config.yaml and .env with your details
        ```

3.  **Python Virtual Environment (Recommended)**:
    It's highly recommended to run the tool's Python scripts within a virtual environment. From the `feature_symphony_tool` directory:
    ```bash
    python3 -m venv .venv
    source .venv/bin/activate  # On Linux/macOS
    # .venv\Scripts\activate    # On Windows
    pip install -r requirements.txt
    ```
    You'll need to activate this venv (`source .venv/bin/activate`) each time you want to run the tool scripts from a new terminal session if you are running them directly. The main wrapper scripts (`run_symphony.sh`, etc.) will attempt to handle this.

4.  **Make Scripts Executable**:
    Ensure the shell scripts are executable:
    ```bash
    cd feature_symphony_tool
    chmod +x bin/dump_repo.sh bin/launch_aiders.sh run_symphony.sh run_single_aider_task.sh
    ```

## `git dump` Script

The `bin/dump_repo.sh` script creates a `repo_contents.txt` file in your project's root. This file contains a concatenated version of most files in your repository, which can be used as context for LLMs.

**Usage**:
Navigate to your main project's root directory (NOT `feature_symphony_tool` itself, unless that's your target project) and run:
```bash
path/to/feature_symphony_tool/bin/dump_repo.sh
```
This will generate `repo_contents.txt` in your current directory (project root).

You can create a Git alias for convenience (e.g., `git dump`) in your global or project-specific `.gitconfig`:
```ini
[alias]
    dump = "!path/to/feature_symphony_tool/bin/dump_repo.sh"
```

## Workflow Overview

(More details will be added as components are built)

1.  **Prepare Symphony XML**: Manually chat with Gemini (using `repo_contents.txt` as context if desired) to break down a large feature. Format the output as specified:
    ```xml
    <!-- Save this as e.g., my_feature_breakdown.xml in your project -->
    <feature_symphony>
    [
        {
            "name": "Implement User Authentication API",
            "description": "Develop backend API endpoints for user registration, login, and logout using JWT."
        },
        {
            "name": "Setup Database Schemas for Users",
            "description": "Define and migrate database schemas for user profiles, credentials, and sessions."
        }
    ]
    </feature_symphony>
    ```

2.  **Run Feature Symphony**:
    From your main project root:
    ```bash
    path/to/feature_symphony_tool/run_symphony.sh path/to/your/my_feature_breakdown.xml
    ```

3.  **Monitor Aider**:
    A `tmux` session will be created, and Aider instances will start working on each feature slice. You can attach to this session to monitor progress.

## Standalone Aider Task

(Details to be added)

---
*Self-correction:* Add `click` to `requirements.txt` for easier CLI creation in Python scripts. Added `.gitignore`. Added `.env.template`. Adjusted `dump_repo.sh` exclusion list and usage instructions in README.

This completes Slice 1. Next, I'll work on the Python orchestrator.
Before that, I'll fill in the placeholder shell scripts with basic structure and argument parsing.

**`feature_symphony_tool/bin/launch_aiders.sh` (Initial Placeholder)**
```bash
#!/usr/bin/env bash
set -euo pipefail

echo "--- Aider Launch Script ---"

if [[ $# -lt 2 ]]; then
  echo "Usage: $0 <RUN_ID> <tasks_json_file_path>"
  echo "Example: $0 myrun123 /path/to/feature_symphony_tool/runs/myrun123/aider_tasks.json"
  exit 1
fi

RUN_ID="$1"
TASKS_JSON_FILE="$2"

echo "RUN_ID: $RUN_ID"
echo "Tasks JSON File: $TASKS_JSON_FILE"

if [ ! -f "$TASKS_JSON_FILE" ]; then
    echo "Error: Tasks JSON file not found at $TASKS_JSON_FILE"
    exit 1
fi

# Placeholder for jq parsing and tmux logic
echo "jq command found: $(command -v jq || echo 'jq not found')"
echo "tmux command found: $(command -v tmux || echo 'tmux not found')"
echo "aider command found: $(command -v aider || echo 'aider not found')"

echo "JSON content:"
cat "$TASKS_JSON_FILE"

echo "TODO: Implement tmux session creation and Aider launch logic."
# Example of what will be parsed:
# tmux_session_name=$(jq -r '.tmux_session_name' "$TASKS_JSON_FILE")
# Number of tasks: $(jq '.tasks | length' "$TASKS_JSON_FILE")

# For each task:
# guide_file=$(jq -r ".tasks[$i].guide_file" "$TASKS_JSON_FILE")
# global_files_str=$(jq -r ".tasks[$i].global_files | join(\" \")" "$TASKS_JSON_FILE")
# prompt_message=$(jq -r ".tasks[$i].prompt" "$TASKS_JSON_FILE")
# aider_cmd="aider $guide_file $global_files_str --message \"$prompt_message\" --yes"

echo "--------------------------"
exit 0
```

**`feature_symphony_tool/run_symphony.sh` (Initial Placeholder)**
```bash
#!/usr/bin/env bash
set -euo pipefail

SCRIPT_DIR_SYMPHONY="$( cd "$( dirname "${BASH_SOURCE[0]}" )" &> /dev/null && pwd )"
TOOL_ROOT="$SCRIPT_DIR_SYMPHONY" # Assuming this script is in feature_symphony_tool/

echo "--- Feature Symphony Orchestrator ---"

if [[ $# -lt 1 ]]; then
  echo "Usage: $0 <path_to_symphony_xml_file>"
  echo "Example: $0 docs/my_feature_plan.xml"
  echo "This script should be run from your main project's root directory."
  exit 1
fi

SYMPHONY_XML_FILE_REL_PATH="$1" # Path relative to current PWD (project root)
SYMPHONY_XML_FILE_ABS_PATH="$(pwd)/$SYMPHONY_XML_FILE_REL_PATH"


CONFIG_FILE_PATH="$TOOL_ROOT/config/config.yaml"
PYTHON_SCRIPT_PATH="$TOOL_ROOT/src/orchestrator.py"
LAUNCH_AIDERS_SCRIPT_PATH="$TOOL_ROOT/bin/launch_aiders.sh"
TOOL_RUN_ARTIFACTS_DIR_NAME="" # Will be read from config

# Check if running from project root (heuristic: .git exists)
if [ ! -d ".git" ]; then
    echo "Warning: This script is intended to be run from your main project's root directory."
    # exit 1 # Or allow continuation with a warning
fi


echo "Symphony XML File (Absolute): $SYMPHONY_XML_FILE_ABS_PATH"
echo "Tool Root: $TOOL_ROOT"
echo "Config File: $CONFIG_FILE_PATH"

if [ ! -f "$SYMPHONY_XML_FILE_ABS_PATH" ]; then
    echo "Error: Symphony XML file not found at $SYMPHONY_XML_FILE_ABS_PATH"
    exit 1
fi
if [ ! -f "$CONFIG_FILE_PATH" ]; then
    echo "Error: Tool configuration file not found at $CONFIG_FILE_PATH"
    echo "Please ensure 'config/config.yaml' exists in $TOOL_ROOT."
    exit 1
fi

# Activate Python virtual environment if it exists within the tool directory
VENV_PATH="$TOOL_ROOT/.venv"
if [ -d "$VENV_PATH" ]; then
    echo "Activating Python virtual environment from $VENV_PATH..."
    # shellcheck source=/dev/null
    source "$VENV_PATH/bin/activate"
else
    echo "Warning: Python virtual environment not found at $VENV_PATH."
    echo "Attempting to use system Python. Ensure dependencies from requirements.txt are installed."
    # Optionally, you could attempt to create and install here, but that's more complex.
fi

# Source .env file from tool directory if it exists
ENV_FILE_PATH="$TOOL_ROOT/.env"
if [ -f "$ENV_FILE_PATH" ]; then
    echo "Sourcing environment variables from $ENV_FILE_PATH..."
    set -a # Automatically export all variables
    # shellcheck source=/dev/null
    source "$ENV_FILE_PATH"
    set +a
else
    echo "Info: .env file not found at $ENV_FILE_PATH. Relying on pre-set environment variables."
fi


RUN_ID=$(date +"%Y%m%d_%H%M%S")
echo "Generated RUN_ID: $RUN_ID"

# Determine tool's internal run artifacts directory from config
# Using a simple grep/awk for now, Python would be more robust for YAML parsing
TOOL_RUN_ARTIFACTS_DIR_NAME=$(grep "tool_run_artifacts_dir:" "$CONFIG_FILE_PATH" | awk '{print $2}' | tr -d '"' | tr -d "'")
if [ -z "$TOOL_RUN_ARTIFACTS_DIR_NAME" ]; then
    echo "Warning: 'tool_run_artifacts_dir' not found or empty in config. Defaulting to no specific artifacts dir."
    # Set a default or handle appropriately
fi

# Define where the orchestrator's JSON output will be stored
ORCHESTRATOR_OUTPUT_JSON=""
if [ -n "$TOOL_RUN_ARTIFACTS_DIR_NAME" ]; then
    # Create run-specific directory inside the tool's structure for its artifacts
    CURRENT_TOOL_RUN_DIR="$TOOL_ROOT/$TOOL_RUN_ARTIFACTS_DIR_NAME/$RUN_ID"
    mkdir -p "$CURRENT_TOOL_RUN_DIR"
    ORCHESTRATOR_OUTPUT_JSON="$CURRENT_TOOL_RUN_DIR/aider_tasks.json"
else
    # Fallback if no artifacts dir, though less ideal. Could use /tmp or project root with .prefix
    ORCHESTRATOR_OUTPUT_JSON="$TOOL_ROOT/aider_tasks_${RUN_ID}.json" # Temporary location
    echo "Warning: Storing orchestrator output JSON in $ORCHESTRATOR_OUTPUT_JSON as tool_run_artifacts_dir is not set."
fi


echo "Running Python orchestrator to generate slice guides and Aider tasks..."
# CWD for orchestrator.py will be the project root.
# It needs to know where config.yaml is (relative to TOOL_ROOT)
# and where to output guides (relative to CWD / project root, as per config)
# and where its own output JSON should go.
python3 "$PYTHON_SCRIPT_PATH" \
    --symphony-xml "$SYMPHONY_XML_FILE_ABS_PATH" \
    --config-file "$CONFIG_FILE_PATH" \
    --run-id "$RUN_ID" \
    --output-json-file "$ORCHESTRATOR_OUTPUT_JSON" \
    --project-root "$(pwd)" # Pass current PWD as project root

if [ $? -ne 0 ]; then
    echo "Error: Python orchestrator script failed."
    exit 1
fi

if [ ! -f "$ORCHESTRATOR_OUTPUT_JSON" ]; then
    echo "Error: Orchestrator did not produce the expected JSON output at $ORCHESTRATOR_OUTPUT_JSON."
    exit 1
fi

echo "Orchestrator finished. Aider tasks defined in: $ORCHESTRATOR_OUTPUT_JSON"
echo "Launching Aider instances via tmux..."

bash "$LAUNCH_AIDERS_SCRIPT_PATH" "$RUN_ID" "$ORCHESTRATOR_OUTPUT_JSON"

if [ $? -ne 0 ]; then
    echo "Error: Aider launch script failed."
    exit 1
fi

echo "--- Feature Symphony Completed ---"
echo "Aider agents should be running in a tmux session."
echo "Attach to session (example): tmux attach-session -t symphony_aider_$RUN_ID"
echo "---------------------------------"
```

**`feature_symphony_tool/run_single_aider_task.sh` (Initial Placeholder)**
```bash
#!/usr/bin/env bash
set -euo pipefail

SCRIPT_DIR_SINGLE="$( cd "$( dirname "${BASH_SOURCE[0]}" )" &> /dev/null && pwd )"
TOOL_ROOT="$SCRIPT_DIR_SINGLE"

echo "--- Standalone Aider Task Runner ---"

if [[ $# -lt 1 ]]; then
  echo "Usage: $0 <path_to_feature_slice_guide.md>"
  echo "Example: $0 docs/feature_guides/my_slice_guide.md"
  echo "This script should be run from your main project's root directory."
  exit 1
fi

FEATURE_SLICE_GUIDE_REL_PATH="$1"
FEATURE_SLICE_GUIDE_ABS_PATH="$(pwd)/$FEATURE_SLICE_GUIDE_REL_PATH"

CONFIG_FILE_PATH="$TOOL_ROOT/config/config.yaml"
LAUNCH_AIDERS_SCRIPT_PATH="$TOOL_ROOT/bin/launch_aiders.sh"
TOOL_RUN_ARTIFACTS_DIR_NAME="" # Will be read from config

# Check if running from project root
if [ ! -d ".git" ]; then
    echo "Warning: This script is intended to be run from your main project's root directory."
fi

echo "Feature Slice Guide (Absolute): $FEATURE_SLICE_GUIDE_ABS_PATH"
echo "Tool Root: $TOOL_ROOT"
echo "Config File: $CONFIG_FILE_PATH"

if [ ! -f "$FEATURE_SLICE_GUIDE_ABS_PATH" ]; then
    echo "Error: Feature slice guide file not found at $FEATURE_SLICE_GUIDE_ABS_PATH"
    exit 1
fi
if [ ! -f "$CONFIG_FILE_PATH" ]; then
    echo "Error: Tool configuration file not found at $CONFIG_FILE_PATH"
    exit 1
fi

# Activate Python virtual environment if it exists within the tool directory
VENV_PATH="$TOOL_ROOT/.venv"
if [ -d "$VENV_PATH" ]; then
    echo "Activating Python virtual environment from $VENV_PATH..."
    # shellcheck source=/dev/null
    source "$VENV_PATH/bin/activate"
fi

# Source .env file from tool directory if it exists
ENV_FILE_PATH="$TOOL_ROOT/.env"
if [ -f "$ENV_FILE_PATH" ]; then
    echo "Sourcing environment variables from $ENV_FILE_PATH..."
    set -a # Automatically export all variables
    # shellcheck source=/dev/null
    source "$ENV_FILE_PATH"
    set +a
fi

RUN_ID="single_task_$(date +"%Y%m%d_%H%M%S")"
echo "Generated RUN_ID for single task: $RUN_ID"


TOOL_RUN_ARTIFACTS_DIR_NAME=$(grep "tool_run_artifacts_dir:" "$CONFIG_FILE_PATH" | awk '{print $2}' | tr -d '"' | tr -d "'")
SINGLE_TASK_JSON=""
if [ -n "$TOOL_RUN_ARTIFACTS_DIR_NAME" ]; then
    CURRENT_TOOL_RUN_DIR="$TOOL_ROOT/$TOOL_RUN_ARTIFACTS_DIR_NAME/$RUN_ID"
    mkdir -p "$CURRENT_TOOL_RUN_DIR"
    SINGLE_TASK_JSON="$CURRENT_TOOL_RUN_DIR/single_aider_task.json"
else
    SINGLE_TASK_JSON="$TOOL_ROOT/single_aider_task_${RUN_ID}.json"
fi

# Read global context files from config.yaml
# This is a bit tricky with pure bash; Python would be better.
# For now, let's assume launch_aiders.sh can read them or they are passed.
# A simple approach: extract from config and build the JSON here in Bash or via a small Python helper.

AIDER_GLOBAL_FILES_STR="" # Placeholder
# TODO: Parse aider_global_context_files from $CONFIG_FILE_PATH
# This part is tricky in pure Bash. Python would be better.
# For a quick start, one could manually list them here or pass them as another arg.
# A better way: `orchestrator.py` could have a mode to generate this JSON for a single file.
# Or, `run_single_aider_task.sh` calls a small Python script to prepare this JSON.

# For now, let's create a simple JSON manually. This needs improvement.
TMUX_SESSION_PREFIX=$(grep "tmux_session_prefix:" "$CONFIG_FILE_PATH" | awk '{print $2}' | tr -d '"' | tr -d "'")
if [ -z "$TMUX_SESSION_PREFIX" ]; then TMUX_SESSION_PREFIX="symphony_aider"; fi # Default

echo "Constructing tasks JSON for single guide..."
# Manually construct JSON for a single task
# Need to correctly escape paths for JSON if they contain special characters.
# Using printf for safer JSON string construction.
# This is still very fragile for aider_global_context_files.
# A Python script to generate this JSON would be much more robust.

# Placeholder for global files - this needs robust parsing from config.yaml
GLOBAL_FILES_JSON_ARRAY="[]" # Default to empty array
# Example of how it might be constructed if aider_global_context_files were simple strings:
# GLOBAL_FILES_LIST=("README.md" "src/main.py") # These should come from config.yaml
# _jq_payload="[]"
# for file in "${GLOBAL_FILES_LIST[@]}"; do
#   _jq_payload=$(echo "$_jq_payload" | jq --arg item "$file" '. + [$item]')
# done
# GLOBAL_FILES_JSON_ARRAY=$_jq_payload

# Let's assume for now that launch_aiders.sh will get global files from config itself when it processes a single task.
# Or, more simply, the orchestrator.py should be the one to generate this JSON even for a single task.
# This script will be simplified if orchestrator.py has a "--single-guide" mode.

# For the purpose of this placeholder, let's assume orchestrator.py is called.
# This means run_single_aider_task.sh will be very similar to run_symphony.sh,
# but orchestrator.py will only process one "feature" (the given guide).

# Re-thinking: run_single_aider_task.sh should directly create a minimal tasks.json
# And launch_aiders.sh should be able to get global_context_files from its own config lookup if not in tasks.json
# This is getting complicated. The `code_builder` model where python generates the full JSON is better.

# New approach for run_single_aider_task.sh:
# It will call `orchestrator.py` with a special flag or different entry point
# to generate the tasks.json for a *single, pre-existing guide*.
# So, orchestrator.py needs:
# main mode: takes XML, generates N guides, generates N tasks in JSON.
# single mode: takes 1 guide path, generates 1 task in JSON (using that guide path).

echo "Calling Python orchestrator in single-guide mode..."
python3 "$TOOL_ROOT/src/orchestrator.py" \
    --single-guide "$FEATURE_SLICE_GUIDE_ABS_PATH" \
    --config-file "$CONFIG_FILE_PATH" \
    --run-id "$RUN_ID" \
    --output-json-file "$SINGLE_TASK_JSON" \
    --project-root "$(pwd)"

if [ $? -ne 0 ]; then
    echo "Error: Python orchestrator script (single-guide mode) failed."
    exit 1
fi
if [ ! -f "$SINGLE_TASK_JSON" ]; then
    echo "Error: Orchestrator (single-guide mode) did not produce JSON output at $SINGLE_TASK_JSON."
    exit 1
fi

echo "Orchestrator finished. Single Aider task defined in: $SINGLE_TASK_JSON"
echo "Launching Aider via tmux..."

bash "$LAUNCH_AIDERS_SCRIPT_PATH" "$RUN_ID" "$SINGLE_TASK_JSON"

if [ $? -ne 0 ]; then
    echo "Error: Aider launch script failed."
    exit 1
fi

echo "--- Standalone Aider Task Completed ---"
echo "Aider agent should be running in a tmux session."
echo "Attach to session (example): tmux attach-session -t ${TMUX_SESSION_PREFIX}_${RUN_ID}"
echo "------------------------------------"
```

This initial scaffolding sets up the project. The shell scripts are placeholders that will drive the Python core and `tmux` operations. The next step (Slice 2) will be to implement `orchestrator.py`.Okay, this is an excellent starting point! We have the basic structure, the `git dump` script, configuration templates, and placeholder shell scripts.

Here's Slice 2, focusing on the core Python orchestration logic.

**Slice 2: Python Orchestration (`orchestrator.py` and `utils.py`)**

**1. `feature_symphony_tool/src/utils.py`**

This file will contain helper functions, starting with loading configuration and making Gemini API calls.

```python
# feature_symphony_tool/src/utils.py
import yaml
import os
import json
from pathlib import Path
import google.generativeai as genai
from slugify import slugify # from python-slugify import slugify

class ConfigError(Exception):
    pass

def load_config(config_file_path: Path) -> dict:
    """Loads configuration from a YAML file."""
    try:
        with open(config_file_path, 'r') as f:
            config_data = yaml.safe_load(f)
        if not config_data:
            raise ConfigError(f"Config file is empty or invalid: {config_file_path}")
        return config_data
    except FileNotFoundError:
        raise ConfigError(f"Configuration file not found: {config_file_path}")
    except yaml.YAMLError as e:
        raise ConfigError(f"Error parsing YAML configuration file: {config_file_path} - {e}")

def get_gemini_api_key(config: dict) -> str:
    """Retrieves Gemini API key from config or environment variable."""
    api_key = config.get('gemini_api_key')
    if not api_key:
        api_key = os.environ.get('GEMINI_API_KEY')
    
    if not api_key:
        raise ConfigError("Gemini API key not found. Set 'gemini_api_key' in config or GEMINI_API_KEY environment variable.")
    return api_key

def call_gemini_api(prompt_text: str, api_key: str, model_name: str) -> str:
    """
    Calls the Gemini API with the given prompt and returns the text response.
    """
    print(f"\nCalling Gemini API with model: {model_name}...")
    # print(f"Prompt (first 100 chars): {prompt_text[:100]}...")

    genai.configure(api_key=api_key)
    
    generation_config = {
        "temperature": 0.7, # Adjust as needed for creativity vs. precision
        "top_p": 1,
        "top_k": 1,
        "max_output_tokens": 8192, # Max for gemini-1.5-pro
    }
    
    safety_settings = [
        {"category": "HARM_CATEGORY_HARASSMENT", "threshold": "BLOCK_MEDIUM_AND_ABOVE"},
        {"category": "HARM_CATEGORY_HATE_SPEECH", "threshold": "BLOCK_MEDIUM_AND_ABOVE"},
        {"category": "HARM_CATEGORY_SEXUALLY_EXPLICIT", "threshold": "BLOCK_MEDIUM_AND_ABOVE"},
        {"category": "HARM_CATEGORY_DANGEROUS_CONTENT", "threshold": "BLOCK_MEDIUM_AND_ABOVE"},
    ]

    try:
        model = genai.GenerativeModel(model_name=model_name,
                                      generation_config=generation_config,
                                      safety_settings=safety_settings)
        
        response = model.generate_content(prompt_text)
        
        if response.candidates and response.candidates[0].content.parts:
            generated_text = "".join(part.text for part in response.candidates[0].content.parts)
            print("Gemini API call successful.")
            return generated_text
        else:
            # print("Gemini API Response (Problem):", response) # Debugging
            # Check for blocked content due to safety settings or other reasons
            if response.prompt_feedback and response.prompt_feedback.block_reason:
                raise Exception(f"Gemini API call blocked. Reason: {response.prompt_feedback.block_reason_message or response.prompt_feedback.block_reason}")
            raise Exception("Gemini API call failed: No content in response or unexpected response structure.")

    except Exception as e:
        print(f"Error calling Gemini API: {e}")
        # print("Full Gemini API response details (if available):")
        # if 'response' in locals() and hasattr(response, 'text'):
        #     print(response.text)
        # elif 'response' in locals():
        #     print(response)
        raise # Re-raise the exception to be caught by the orchestrator

def generate_slug(text: str) -> str:
    """Generates a URL-friendly slug from text."""
    return slugify(text, max_length=50, word_boundary=True, separator="_")

```

**2. `feature_symphony_tool/src/orchestrator.py`**

This is the core script. It will handle parsing the XML, generating guides, and preparing the JSON for `launch_aiders.sh`.

```python
# feature_symphony_tool/src/orchestrator.py
import xml.etree.ElementTree as ET
import json
import os
import click
from pathlib import Path
import sys

# Ensure src directory is in Python path to import utils
SCRIPT_DIR_ORCH = Path(__file__).resolve().parent
sys.path.append(str(SCRIPT_DIR_ORCH.parent))

from src.utils import load_config, get_gemini_api_key, call_gemini_api, generate_slug, ConfigError

DEFAULT_AIDER_PROMPT = "Please implement this guide."

def parse_symphony_xml(xml_filepath: Path) -> list[dict]:
    """Parses the feature symphony XML file and extracts feature details."""
    try:
        print(f"Parsing symphony XML file: {xml_filepath}")
        tree = ET.parse(xml_filepath)
        root = tree.getroot()
        
        if root.tag != "feature_symphony":
            raise ValueError("Root XML tag must be <feature_symphony>")
            
        # The content inside <feature_symphony> is expected to be a JSON string
        json_string = root.text.strip()
        if not json_string:
            raise ValueError("No JSON content found within <feature_symphony> tags.")
            
        features = json.loads(json_string)
        if not isinstance(features, list):
            raise ValueError("Parsed JSON content is not a list.")
        
        # Validate feature structure
        for i, feature in enumerate(features):
            if not isinstance(feature, dict) or "name" not in feature or "description" not in feature:
                raise ValueError(f"Invalid feature structure at index {i}. Each feature must be a dict with 'name' and 'description'. Found: {feature}")
        
        print(f"Successfully parsed {len(features)} features from XML.")
        return features
    except FileNotFoundError:
        print(f"Error: Symphony XML file not found at {xml_filepath}")
        raise
    except ET.ParseError as e:
        print(f"Error parsing XML file {xml_filepath}: {e}")
        raise
    except json.JSONDecodeError as e:
        print(f"Error parsing JSON content within XML {xml_filepath}: {e}")
        raise
    except ValueError as e:
        print(f"Error validating XML content {xml_filepath}: {e}")
        raise


def generate_feature_slice_guide(
    feature_info: dict, 
    gemini_api_key: str, 
    gemini_model: str,
    project_root: Path,
    repo_context_file: Path = None # Optional repo_contents.txt
) -> str:
    """Generates a detailed implementation guide for a single feature slice using Gemini."""
    feature_name = feature_info["name"]
    feature_description = feature_info["description"]
    
    print(f"Generating feature slice guide for: '{feature_name}'...")
    
    context_text = ""
    if repo_context_file and repo_context_file.exists():
        print(f"Including repository context from: {repo_context_file}")
        context_text = f"\n\n--- Repository Context ---\n{repo_context_file.read_text()}\n--- End Repository Context ---"
    elif repo_context_file:
        print(f"Warning: Specified repository context file not found: {repo_context_file}")

    prompt = f"""
You are an expert software architect and senior developer. Your task is to generate a detailed, step-by-step implementation guide for the following software feature. This guide will be used by an AI coding assistant (Aider) to implement the feature.

Feature Name: {feature_name}
Feature Description: {feature_description}

Please provide a comprehensive guide that includes:
1.  **Objective**: A clear statement of what this feature slice aims to achieve.
2.  **Key Files to Create/Modify**: List specific file paths (relative to the project root: "{project_root}") that will likely be involved. If new files are needed, suggest their names and locations.
3.  **Detailed Implementation Steps**: Break down the implementation into small, actionable steps. For each step, describe what needs to be done. Be specific. If there's code to be written, provide examples or clear instructions.
4.  **Data Structures/Schemas (if applicable)**: Define any new data models, Pydantic schemas, database table structures, or important type definitions.
5.  **API Endpoints (if applicable)**: Specify routes, HTTP methods, request/response bodies for any APIs.
6.  **Important Considerations/Edge Cases**: Highlight any potential challenges, dependencies, or edge cases the AI assistant should be aware of.
7.  **Testing Suggestions**: Briefly outline what kind of tests (unit, integration) would be appropriate for this feature slice and what they should cover.

The AI assistant (Aider) will be operating within the project root: "{project_root}".
Ensure all file paths mentioned are relative to this root.
The output should be a clear, well-structured markdown document.
{context_text}

Begin the guide now:
"""
    
    try:
        guide_content = call_gemini_api(prompt, gemini_api_key, gemini_model)
        return guide_content
    except Exception as e:
        print(f"Failed to generate guide for '{feature_name}': {e}")
        # Return a placeholder or re-raise if critical
        return f"# FAILED GUIDE: {feature_name}\n\nError during generation: {e}"


def save_guide(guide_content: str, output_dir: Path, feature_name: str) -> Path:
    """Saves the generated guide to a file."""
    guide_slug = generate_slug(feature_name)
    guide_filename = f"feature_slice_guide_{guide_slug}.md"
    output_filepath = output_dir / guide_filename
    
    output_dir.mkdir(parents=True, exist_ok=True)
    
    with open(output_filepath, 'w', encoding='utf-8') as f:
        f.write(guide_content)
    print(f"Saved guide to: {output_filepath}")
    return output_filepath


def prepare_aider_tasks_json(
    tasks: list, 
    tmux_session_prefix: str, 
    run_id: str
) -> dict:
    """Prepares the JSON structure for launch_aiders.sh."""
    return {
        "tmux_session_name": f"{tmux_session_prefix}_{run_id}",
        "tasks": tasks
    }

@click.command()
@click.option('--config-file', required=True, type=click.Path(exists=True, dir_okay=False, path_type=Path), help="Path to the config.yaml file.")
@click.option('--run-id', required=True, type=str, help="Unique ID for this run.")
@click.option('--output-json-file', required=True, type=click.Path(dir_okay=False, writable=True, path_type=Path), help="Path to save the output Aider tasks JSON file.")
@click.option('--project-root', required=True, type=click.Path(exists=True, file_okay=False, path_type=Path), help="Absolute path to the root of the target project where Aider will run.")
@click.option('--symphony-xml', type=click.Path(exists=True, dir_okay=False, path_type=Path), help="Path to the symphony XML file (for full workflow).")
@click.option('--single-guide', type=click.Path(exists=True, dir_okay=False, path_type=Path), help="Path to a pre-existing single feature slice guide (for single task workflow).")
@click.option('--repo-context-file', type=click.Path(exists=False, dir_okay=False, path_type=Path), default=None, help="Optional path to a repo_contents.txt file for additional context during guide generation. Relative to project_root.")

def main(
    config_file: Path, 
    run_id: str, 
    output_json_file: Path,
    project_root: Path,
    symphony_xml: Path, 
    single_guide: Path,
    repo_context_file: Path
    ):
    """
    Orchestrates feature slice guide generation and prepares Aider tasks.
    Run from the main project's root directory.
    The `config_file` path should be absolute or relative to feature_symphony_tool dir.
    The `guides_output_directory` in config.yaml is relative to `project_root`.
    """
    try:
        print(f"--- Feature Symphony Orchestrator ---")
        print(f"Run ID: {run_id}")
        print(f"Config File: {config_file}")
        print(f"Project Root: {project_root}")
        
        config = load_config(config_file)
        gemini_api_key = get_gemini_api_key(config)
        gemini_model = config.get('gemini_model_guide_generation', 'gemini-1.5-pro-latest')
        
        # Output directory for guides, relative to the project_root
        guides_output_dir_rel = config.get('guides_output_directory', 'docs/feature_guides')
        guides_output_dir_abs = project_root / guides_output_dir_rel
        guides_output_dir_abs.mkdir(parents=True, exist_ok=True)
        print(f"Generated guides will be saved to: {guides_output_dir_abs}")

        # Global context files for Aider, relative to project_root
        aider_global_context_files_rel = config.get('aider_global_context_files', [])
        aider_global_context_files_abs = [str(project_root / f) for f in aider_global_context_files_rel]
        
        # Resolve repo_context_file path if provided
        actual_repo_context_file = None
        if repo_context_file:
            actual_repo_context_file = project_root / repo_context_file
            if not actual_repo_context_file.exists():
                print(f"Warning: repo-context-file '{actual_repo_context_file}' not found. Proceeding without it.")
                actual_repo_context_file = None


        aider_tasks = []

        if symphony_xml:
            print(f"Processing symphony XML: {symphony_xml}")
            features = parse_symphony_xml(symphony_xml)
            for feature_info in features:
                guide_content = generate_feature_slice_guide(
                    feature_info, 
                    gemini_api_key, 
                    gemini_model,
                    project_root,
                    actual_repo_context_file
                )
                saved_guide_path_abs = save_guide(guide_content, guides_output_dir_abs, feature_info['name'])
                # Store path relative to project_root for Aider, as Aider runs from there
                saved_guide_path_rel = saved_guide_path_abs.relative_to(project_root)
                
                aider_tasks.append({
                    "guide_file": str(saved_guide_path_rel), # Path relative to project_root
                    "global_files": aider_global_context_files_rel, # Paths relative to project_root
                    "prompt": DEFAULT_AIDER_PROMPT,
                    "description": f"Implement feature: {feature_info['name']}"
                })
        elif single_guide:
            print(f"Processing single pre-existing guide: {single_guide}")
            # Ensure single_guide is relative to project_root for consistency in tasks.json
            single_guide_rel = single_guide.relative_to(project_root) if single_guide.is_absolute() else single_guide
            
            aider_tasks.append({
                "guide_file": str(single_guide_rel), # Path relative to project_root
                "global_files": aider_global_context_files_rel, # Paths relative to project_root
                "prompt": DEFAULT_AIDER_PROMPT,
                "description": f"Implement guide: {single_guide.name}"
            })
        else:
            raise click.UsageError("Either --symphony-xml or --single-guide must be provided.")

        tmux_session_prefix = config.get('tmux_session_prefix', 'symphony_aider')
        output_data = prepare_aider_tasks_json(aider_tasks, tmux_session_prefix, run_id)
        
        # Ensure output JSON directory exists
        output_json_file.parent.mkdir(parents=True, exist_ok=True)
        with open(output_json_file, 'w') as f:
            json.dump(output_data, f, indent=2)
        
        print(f"Aider tasks JSON saved to: {output_json_file}")
        print("--- Orchestrator Finished Successfully ---")

    except ConfigError as e:
        print(f"Configuration Error: {e}", file=sys.stderr)
        sys.exit(1)
    except FileNotFoundError as e:
        print(f"File Not Found Error: {e}", file=sys.stderr)
        sys.exit(1)
    except ET.ParseError as e: # XML parsing error
        print(f"XML Parsing Error: {e}", file=sys.stderr)
        sys.exit(1)
    except json.JSONDecodeError as e: # JSON parsing error
        print(f"JSON Parsing Error: {e}", file=sys.stderr)
        sys.exit(1)
    except ValueError as e: # Other value errors (e.g. from XML/JSON content validation)
        print(f"Value Error: {e}", file=sys.stderr)
        sys.exit(1)
    except click.UsageError as e:
        print(f"Usage Error: {e}", file=sys.stderr)
        sys.exit(2) # click's standard exit code for usage errors
    except Exception as e:
        print(f"An unexpected error occurred: {e}", file=sys.stderr)
        import traceback
        traceback.print_exc()
        sys.exit(1)

if __name__ == '__main__':
    main()
```

**Key changes and considerations for Slice 2:**
*   **`utils.py`**: Contains `load_config`, `get_gemini_api_key`, `call_gemini_api`, and `generate_slug`. The Gemini API call uses the official `google-generativeai` library.
*   **`orchestrator.py`**:
    *   Uses `click` for command-line argument parsing. This makes it more robust.
    *   `parse_symphony_xml`: Parses the XML to extract the JSON list of features. Added validation for XML structure and JSON content.
    *   `generate_feature_slice_guide`: Constructs a detailed prompt for Gemini. Includes an option to pass a `repo_context_file` (e.g., `repo_contents.txt`).
    *   `save_guide`: Saves the generated markdown guide with a slugified filename.
    *   `prepare_aider_tasks_json`: Creates the JSON structure that `launch_aiders.sh` will consume.
    *   `main()` function:
        *   Handles two modes: processing a symphony XML or a single pre-existing guide.
        *   Resolves paths correctly: `guides_output_directory` from config is relative to the `project_root` passed to the script. `aider_global_context_files` are also treated as relative to `project_root`.
        *   The `guide_file` and `global_files` paths in the output JSON are relative to `project_root`, as Aider will likely run with `project_root` as its CWD.
        *   Added error handling for common issues.
*   **Prompt Engineering**: The prompt for Gemini in `generate_feature_slice_guide` is crucial. I've included placeholders for key sections a good guide should have. This will likely need iteration.
*   **Path Management**: Paths are a bit tricky.
    *   The `run_symphony.sh` script is run from the *target project's root*.
    *   It calls `orchestrator.py`, passing the *absolute path* to `config.yaml` (which is inside `feature_symphony_tool/config/`) and the *absolute path* to the target project root.
    *   `orchestrator.py` then interprets `guides_output_directory` and `aider_global_context_files` from `config.yaml` as being relative to the `project_root`.
*   **Repo Context**: Added an optional `--repo-context-file` argument to `orchestrator.py` so that `repo_contents.txt` can be passed to Gemini when generating slice guides.

This Python core should now be able to:
1.  Read the configuration.
2.  Parse the user's XML feature breakdown.
3.  Call Gemini to generate detailed markdown guides for each feature.
4.  Save these guides to the location specified in `config.yaml` (relative to the target project root).
5.  Output a JSON file detailing the tasks (guide paths, global files) for the Aider launch script.
6.  Handle a single pre-existing guide to prepare a task JSON for it.

Next up (Slice 3) will be the `launch_aiders.sh` script to consume this JSON and manage `tmux` and `aider`.